# ============================================================
# Stage 2: Fine-tuning Configuration
# ============================================================

model:
  num_classes: 2 # Normal vs Arrhythmia (binary)
  freeze_layers: 4 # freeze first 4 of 6 encoder blocks
  dropout: 0.5 # aggressive dropout for small data

training:
  epochs: 50
  batch_size: 32
  learning_rate: 1.0e-5 # 10x smaller than pre-training
  weight_decay: 0.01
  patience: 10 # early stopping patience
  label_smoothing: 0.1
  num_workers: 2

few_shot:
  n_shots: [10, 50, 100, 1000]
  seeds: [42, 123, 456, 789, 1011]

logging:
  wandb_project: "ecg-ssl-research"
  wandb_group: "finetuning"
